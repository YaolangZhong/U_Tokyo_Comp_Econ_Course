{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b126825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mdp import MDP\n",
    "from algos import VFI, PFI, HowardPFI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb5e1f6",
   "metadata": {},
   "source": [
    "# The 3-state Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b24f85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards (R):\n",
      " [[-inf   1.   2.]\n",
      " [  0. -inf   2.]\n",
      " [  0.   1. -inf]]\n",
      "Transitions from s0 with a1/a2:\n",
      "P[0,1,:] = [0. 1. 0.]   P[0,2,:] = [0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "names_s = [\"s0\", \"s1\", \"s2\"]\n",
    "names_a = [\"a0\", \"a1\", \"a2\"]\n",
    "\n",
    "S, A = 3, 3\n",
    "P = np.zeros((S, A, S))\n",
    "R = -np.inf * np.ones((S, A))  # default: all actions forbidden\n",
    "\n",
    "# s0: a1 -> s1 (r=1), a2 -> s2 (r=2)\n",
    "P[0, 1, 1] = 1.0; R[0, 1] = 1.0\n",
    "P[0, 2, 2] = 1.0; R[0, 2] = 2.0\n",
    "\n",
    "# s1: a0 -> s0 (r=0), a2 -> s2 (r=2)\n",
    "P[1, 0, 0] = 1.0; R[1, 0] = 0.0\n",
    "P[1, 2, 2] = 1.0; R[1, 2] = 2.0\n",
    "\n",
    "# s2: a0 -> s0 (r=0), a1 -> s1 (r=1)\n",
    "P[2, 0, 0] = 1.0; R[2, 0] = 0.0\n",
    "P[2, 1, 1] = 1.0; R[2, 1] = 1.0\n",
    "\n",
    "beta = 0.9\n",
    "\n",
    "mdp = MDP(S=S, A=A, P=P, R=R, beta=beta, names_s=names_s, names_a=names_a)\n",
    "\n",
    "# (Optional) quick sanity print\n",
    "print(\"Rewards (R):\\n\", R)\n",
    "print(\"Transitions from s0 with a1/a2:\\nP[0,1,:] =\", P[0,1,:], \"  P[0,2,:] =\", P[0,2,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3de3fdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SOLVING MDP WITH THREE ALGORITHMS\n",
      "============================================================\n",
      "\n",
      "1. Value Function Iteration (VFI)\n",
      "----------------------------------------\n",
      "Iterations: 183\n",
      "Time: 0.004031 seconds\n",
      "Value function: [15.26315783 15.26315783 14.73684204]\n",
      "Policy: [2 2 1]\n",
      "Policy (named): ['a2', 'a2', 'a1']\n",
      "\n",
      "2. Policy Function Iteration (PFI)\n",
      "----------------------------------------\n",
      "Time: 0.000657 seconds\n",
      "Value function: [15.26315785 15.26315785 14.73684207]\n",
      "Policy: [2 2 1]\n",
      "Policy (named): ['a2', 'a2', 'a1']\n",
      "\n",
      "3. Howard PFI (m=1, Modified Policy Iteration)\n",
      "----------------------------------------\n",
      "Time: 0.000117 seconds\n",
      "Value function: [2.  2.  2.8]\n",
      "Policy: [2 2 1]\n",
      "Policy (named): ['a2', 'a2', 'a1']\n",
      "\n",
      "4. Howard PFI (m=5)\n",
      "----------------------------------------\n",
      "Time: 0.000126 seconds\n",
      "Value function: [9.55380332 9.55380332 9.59842299]\n",
      "Policy: [2 2 1]\n",
      "Policy (named): ['a2', 'a2', 'a1']\n",
      "\n",
      "5. Howard PFI (m=10)\n",
      "----------------------------------------\n",
      "Time: 0.000140 seconds\n",
      "Value function: [13.27242905 13.27242905 12.94518614]\n",
      "Policy: [2 2 1]\n",
      "Policy (named): ['a2', 'a2', 'a1']\n",
      "\n",
      "============================================================\n",
      "COMPARISON SUMMARY\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Solve the MDP using all three algorithms\n",
    "print(\"=\"*60)\n",
    "print(\"SOLVING MDP WITH THREE ALGORITHMS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Value Function Iteration (VFI)\n",
    "print(\"\\n1. Value Function Iteration (VFI)\")\n",
    "print(\"-\" * 40)\n",
    "start_time = time.time()\n",
    "vfi = VFI(mdp, tol=1e-8, max_iter=1000)\n",
    "V_vfi, pi_vfi, iterations_vfi = vfi.solve()\n",
    "time_vfi = time.time() - start_time\n",
    "\n",
    "print(f\"Iterations: {iterations_vfi}\")\n",
    "print(f\"Time: {time_vfi:.6f} seconds\")\n",
    "print(f\"Value function: {V_vfi}\")\n",
    "print(f\"Policy: {pi_vfi}\")\n",
    "print(f\"Policy (named): {[names_a[a] for a in pi_vfi]}\")\n",
    "\n",
    "# 2. Policy Function Iteration (PFI) - converges to tolerance\n",
    "print(\"\\n2. Policy Function Iteration (PFI)\")\n",
    "print(\"-\" * 40)\n",
    "start_time = time.time()\n",
    "pfi = PFI(mdp, eval_tol=1e-8, max_eval_iter=1000, max_outer=100)\n",
    "V_pfi, pi_pfi = pfi.solve()\n",
    "time_pfi = time.time() - start_time\n",
    "\n",
    "print(f\"Time: {time_pfi:.6f} seconds\")\n",
    "print(f\"Value function: {V_pfi}\")\n",
    "print(f\"Policy: {pi_pfi}\")\n",
    "print(f\"Policy (named): {[names_a[a] for a in pi_pfi]}\")\n",
    "\n",
    "# 3. Howard PFI with m=1 (Modified Policy Iteration)\n",
    "print(\"\\n3. Howard PFI (m=1, Modified Policy Iteration)\")\n",
    "print(\"-\" * 40)\n",
    "start_time = time.time()\n",
    "howard_pfi_1 = HowardPFI(mdp, m=1, max_outer=100)\n",
    "V_howard1, pi_howard1 = howard_pfi_1.solve()\n",
    "time_howard1 = time.time() - start_time\n",
    "\n",
    "print(f\"Time: {time_howard1:.6f} seconds\")\n",
    "print(f\"Value function: {V_howard1}\")\n",
    "print(f\"Policy: {pi_howard1}\")\n",
    "print(f\"Policy (named): {[names_a[a] for a in pi_howard1]}\")\n",
    "\n",
    "# 4. Howard PFI with m=5\n",
    "print(\"\\n4. Howard PFI (m=5)\")\n",
    "print(\"-\" * 40)\n",
    "start_time = time.time()\n",
    "howard_pfi_5 = HowardPFI(mdp, m=5, max_outer=100)\n",
    "V_howard5, pi_howard5 = howard_pfi_5.solve()\n",
    "time_howard5 = time.time() - start_time\n",
    "\n",
    "print(f\"Time: {time_howard5:.6f} seconds\")\n",
    "print(f\"Value function: {V_howard5}\")\n",
    "print(f\"Policy: {pi_howard5}\")\n",
    "print(f\"Policy (named): {[names_a[a] for a in pi_howard5]}\")\n",
    "\n",
    "# 5. Howard PFI with m=10\n",
    "print(\"\\n5. Howard PFI (m=10)\")\n",
    "print(\"-\" * 40)\n",
    "start_time = time.time()\n",
    "howard_pfi_10 = HowardPFI(mdp, m=10, max_outer=100)\n",
    "V_howard10, pi_howard10 = howard_pfi_10.solve()\n",
    "time_howard10 = time.time() - start_time\n",
    "\n",
    "print(f\"Time: {time_howard10:.6f} seconds\")\n",
    "print(f\"Value function: {V_howard10}\")\n",
    "print(f\"Policy: {pi_howard10}\")\n",
    "print(f\"Policy (named): {[names_a[a] for a in pi_howard10]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON SUMMARY\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a802272f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMING COMPARISON:\n",
      "------------------------------\n",
      "VFI         : 0.004031 seconds\n",
      "PFI         : 0.000657 seconds\n",
      "Howard(m=1) : 0.000117 seconds\n",
      "Howard(m=5) : 0.000126 seconds\n",
      "Howard(m=10): 0.000140 seconds\n",
      "\n",
      "Fastest: Howard(m=1) (0.000117s)\n",
      "Slowest: VFI (0.004031s)\n",
      "\n",
      "VALUE FUNCTION COMPARISON:\n",
      "------------------------------\n",
      "All value functions should be identical (up to numerical precision)\n",
      "VFI         : [15.26315783 15.26315783 14.73684204]\n",
      "PFI         : [15.26315785 15.26315785 14.73684207]\n",
      "Howard(m=1) : [2.  2.  2.8]\n",
      "Howard(m=5) : [9.55380332 9.55380332 9.59842299]\n",
      "Howard(m=10): [13.27242905 13.27242905 12.94518614]\n",
      "\n",
      "All value functions equal (tol=1e-06): False\n",
      "\n",
      "POLICY COMPARISON:\n",
      "------------------------------\n",
      "All policies should be identical\n",
      "VFI         : [2 2 1] -> ['a2', 'a2', 'a1']\n",
      "PFI         : [2 2 1] -> ['a2', 'a2', 'a1']\n",
      "Howard(m=1) : [2 2 1] -> ['a2', 'a2', 'a1']\n",
      "Howard(m=5) : [2 2 1] -> ['a2', 'a2', 'a1']\n",
      "Howard(m=10): [2 2 1] -> ['a2', 'a2', 'a1']\n",
      "\n",
      "All policies identical: True\n",
      "\n",
      "OPTIMAL POLICY INTERPRETATION:\n",
      "------------------------------\n",
      "State s0: Take action a2\n",
      "State s1: Take action a2\n",
      "State s2: Take action a1\n",
      "\n",
      "ALGORITHM CHARACTERISTICS:\n",
      "------------------------------\n",
      "• VFI: Iterates value function until convergence\n",
      "• PFI: Policy evaluation to convergence + policy improvement\n",
      "• Howard(m=1): 1 step policy evaluation + policy improvement (Modified PI)\n",
      "• Howard(m=5): 5 steps policy evaluation + policy improvement\n",
      "• Howard(m=10): 10 steps policy evaluation + policy improvement\n"
     ]
    }
   ],
   "source": [
    "# Comparison of results and timing\n",
    "algorithms = [\"VFI\", \"PFI\", \"Howard(m=1)\", \"Howard(m=5)\", \"Howard(m=10)\"]\n",
    "times = [time_vfi, time_pfi, time_howard1, time_howard5, time_howard10]\n",
    "value_functions = [V_vfi, V_pfi, V_howard1, V_howard5, V_howard10]\n",
    "policies = [pi_vfi, pi_pfi, pi_howard1, pi_howard5, pi_howard10]\n",
    "\n",
    "print(\"TIMING COMPARISON:\")\n",
    "print(\"-\" * 30)\n",
    "for i, (alg, t) in enumerate(zip(algorithms, times)):\n",
    "    print(f\"{alg:12s}: {t:.6f} seconds\")\n",
    "\n",
    "print(f\"\\nFastest: {algorithms[np.argmin(times)]} ({min(times):.6f}s)\")\n",
    "print(f\"Slowest: {algorithms[np.argmax(times)]} ({max(times):.6f}s)\")\n",
    "\n",
    "print(\"\\nVALUE FUNCTION COMPARISON:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"All value functions should be identical (up to numerical precision)\")\n",
    "for i, (alg, V) in enumerate(zip(algorithms, value_functions)):\n",
    "    print(f\"{alg:12s}: {V}\")\n",
    "\n",
    "# Check if all value functions are approximately equal\n",
    "all_equal = True\n",
    "tolerance = 1e-6\n",
    "for i in range(1, len(value_functions)):\n",
    "    if not np.allclose(value_functions[0], value_functions[i], atol=tolerance):\n",
    "        all_equal = False\n",
    "        break\n",
    "\n",
    "print(f\"\\nAll value functions equal (tol={tolerance}): {all_equal}\")\n",
    "\n",
    "print(\"\\nPOLICY COMPARISON:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"All policies should be identical\")\n",
    "for i, (alg, pi) in enumerate(zip(algorithms, policies)):\n",
    "    policy_names = [names_a[a] for a in pi]\n",
    "    print(f\"{alg:12s}: {pi} -> {policy_names}\")\n",
    "\n",
    "# Check if all policies are identical\n",
    "policies_equal = all(np.array_equal(policies[0], pi) for pi in policies[1:])\n",
    "print(f\"\\nAll policies identical: {policies_equal}\")\n",
    "\n",
    "print(\"\\nOPTIMAL POLICY INTERPRETATION:\")\n",
    "print(\"-\" * 30)\n",
    "optimal_policy = [names_a[a] for a in policies[0]]  # Use VFI result as reference\n",
    "for s in range(S):\n",
    "    print(f\"State {names_s[s]}: Take action {optimal_policy[s]}\")\n",
    "\n",
    "print(\"\\nALGORITHM CHARACTERISTICS:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"• VFI: Iterates value function until convergence\")\n",
    "print(\"• PFI: Policy evaluation to convergence + policy improvement\")\n",
    "print(\"• Howard(m=1): 1 step policy evaluation + policy improvement (Modified PI)\")\n",
    "print(\"• Howard(m=5): 5 steps policy evaluation + policy improvement\")\n",
    "print(\"• Howard(m=10): 10 steps policy evaluation + policy improvement\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28308e5",
   "metadata": {},
   "source": [
    "# Economics Model: Consumption-Savings Problem\n",
    "\n",
    "We'll model a classic consumption-savings problem as an MDP:\n",
    "\n",
    "## Economic Setup\n",
    "- **Agent**: Has wealth and must decide how much to consume vs save\n",
    "- **States**: Discrete wealth levels $(0, 1, 2, ..., W_max)$\n",
    "- **Actions**: Consumption levels (0, 1, 2, ..., up to current wealth)\n",
    "- **Utility**: Logarithmic utility from consumption: $u(c) = log(c + ε)$ where $ε$ prevents $log(0)$\n",
    "- **Wealth Evolution**: $W' = (W - c) × R$, where $R$ is gross return on savings\n",
    "- **Discount Factor**: $β$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fe347b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_consumption_savings_mdp(W_max=10, return_rate=1.05, beta=0.95, epsilon=0.01):\n",
    "    \"\"\"\n",
    "    Create a consumption-savings MDP.\n",
    "    \n",
    "    Parameters:\n",
    "    - W_max: Maximum wealth level (states are 0, 1, ..., W_max)\n",
    "    - return_rate: Gross return on savings (e.g., 1.05 = 5% return)\n",
    "    - beta: Discount factor\n",
    "    - epsilon: Small constant to avoid log(0) in utility\n",
    "    \n",
    "    Returns:\n",
    "    - MDP object\n",
    "    \"\"\"\n",
    "    S = W_max + 1  # States: 0, 1, 2, ..., W_max\n",
    "    A = W_max + 1  # Actions: 0, 1, 2, ..., W_max (max possible consumption)\n",
    "    \n",
    "    # Initialize transition probabilities and rewards\n",
    "    P = np.zeros((S, A, S))\n",
    "    R = -np.inf * np.ones((S, A))  # Default: forbidden actions\n",
    "    \n",
    "    # State and action names for interpretation\n",
    "    names_s = [f\"W={w}\" for w in range(S)]\n",
    "    names_a = [f\"c={c}\" for c in range(A)]\n",
    "    \n",
    "    for w in range(S):  # Current wealth\n",
    "        for c in range(A):  # Consumption choice\n",
    "            if c <= w:  # Can only consume up to current wealth\n",
    "                # Utility from consumption (logarithmic)\n",
    "                utility = np.log(c + epsilon)\n",
    "                R[w, c] = utility\n",
    "                \n",
    "                # Next period wealth: (current_wealth - consumption) * return_rate\n",
    "                savings = w - c\n",
    "                next_wealth = int(np.round(savings * return_rate))\n",
    "                \n",
    "                # Ensure next_wealth stays within bounds\n",
    "                next_wealth = min(next_wealth, W_max)\n",
    "                next_wealth = max(next_wealth, 0)\n",
    "                \n",
    "                # Deterministic transition to next wealth level\n",
    "                P[w, c, next_wealth] = 1.0\n",
    "    \n",
    "    return MDP(S=S, A=A, P=P, R=R, beta=beta, names_s=names_s, names_a=names_a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "506b27c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States: 9 wealth levels (0 to 8)\n",
      "Actions: Up to 9 consumption choices per state\n",
      "Return on savings: 110.0%\n",
      "Discount factor: 0.9\n",
      "Utility function: u(c) = log(c + 0.1)\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "W_max = 8      # Maximum wealth level\n",
    "R_return = 1.1 # 10% return on savings\n",
    "beta = 0.9     # Discount factor\n",
    "epsilon = 0.1  # Utility parameter\n",
    "\n",
    "econ_mdp = create_consumption_savings_mdp(W_max=W_max, return_rate=R_return, beta=beta, epsilon=epsilon)\n",
    "\n",
    "print(f\"States: {econ_mdp.S} wealth levels (0 to {W_max})\")\n",
    "print(f\"Actions: Up to {econ_mdp.A} consumption choices per state\")\n",
    "print(f\"Return on savings: {R_return:.1%}\")\n",
    "print(f\"Discount factor: {beta}\")\n",
    "print(f\"Utility function: u(c) = log(c + {epsilon})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a8bc23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SOLVING CONSUMPTION-SAVINGS MDP WITH THREE ALGORITHMS\n",
      "============================================================\n",
      "\n",
      "1. Value Function Iteration (VFI)\n",
      "----------------------------------------\n",
      "Iterations: 184\n",
      "Time: 0.021442 seconds\n",
      "Value function: [-23.02585084 -20.62795557 -18.46984982 -16.52755465 -14.779489\n",
      "  -1.44479347   0.9531018    1.59972897   2.18169342]\n",
      "Optimal consumption policy: [0 1 1 1 1 0 1 2 2]\n",
      "\n",
      "2. Policy Function Iteration (PFI)\n",
      "----------------------------------------\n",
      "Time: 0.004425 seconds\n",
      "Value function: [-23.02585093 -20.62795566 -18.46984991 -16.52755474 -14.77948909\n",
      "  -1.44479354   0.95310173   1.5997289    2.18169336]\n",
      "Optimal consumption policy: [0 1 1 1 1 0 1 2 2]\n",
      "\n",
      "3.1. Howard PFI (m=1)\n",
      "----------------------------------------\n",
      "Time: 0.001497 seconds\n",
      "Value function: [-18.28503241 -16.36121899 -14.62978691 -13.07149804 -11.66903806\n",
      "  -2.04139893   0.35649634   1.06278405   1.69844299]\n",
      "Optimal consumption policy: [0 1 1 1 1 0 1 2 2]\n",
      "\n",
      "3.3. Howard PFI (m=3)\n",
      "----------------------------------------\n",
      "Time: 0.001680 seconds\n",
      "Value function: [-22.04975844 -19.74947242 -17.67921499 -15.81598331 -14.1390748\n",
      "  -1.99852524   0.39937003   1.10137037   1.73317068]\n",
      "Optimal consumption policy: [0 1 1 1 1 0 1 2 2]\n",
      "\n",
      "3.5. Howard PFI (m=5)\n",
      "----------------------------------------\n",
      "Time: 0.001048 seconds\n",
      "Value function: [-22.68550852 -20.32164749 -18.19417256 -16.27944513 -14.55619043\n",
      "  -1.86911785   0.52877742   1.21783702   1.83799066]\n",
      "Optimal consumption policy: [0 1 1 1 1 0 1 2 2]\n",
      "\n",
      "============================================================\n",
      "ECONOMIC ANALYSIS OF RESULTS\n",
      "============================================================\n",
      "\n",
      "1. ALGORITHM PERFORMANCE COMPARISON:\n",
      "--------------------------------------------------\n",
      "VFI         : 0.021442 seconds\n",
      "PFI         : 0.004425 seconds\n",
      "Howard(m=1) : 0.001497 seconds\n",
      "Howard(m=3) : 0.001680 seconds\n",
      "Howard(m=5) : 0.001048 seconds\n",
      "\n",
      "Fastest: Howard(m=5) (0.001048s)\n",
      "Slowest: VFI (0.021442s)\n",
      "\n",
      "2. VALUE FUNCTION CONVERGENCE:\n",
      "--------------------------------------------------\n",
      "Value functions (lifetime utility for each wealth level):\n",
      "VFI         : [-23.02585084 -20.62795557 -18.46984982 -16.52755465 -14.779489\n",
      "  -1.44479347   0.9531018    1.59972897   2.18169342]\n",
      "PFI         : [-23.02585093 -20.62795566 -18.46984991 -16.52755474 -14.77948909\n",
      "  -1.44479354   0.95310173   1.5997289    2.18169336]\n",
      "Howard(m=1) : [-18.28503241 -16.36121899 -14.62978691 -13.07149804 -11.66903806\n",
      "  -2.04139893   0.35649634   1.06278405   1.69844299]\n",
      "Howard(m=3) : [-22.04975844 -19.74947242 -17.67921499 -15.81598331 -14.1390748\n",
      "  -1.99852524   0.39937003   1.10137037   1.73317068]\n",
      "Howard(m=5) : [-22.68550852 -20.32164749 -18.19417256 -16.27944513 -14.55619043\n",
      "  -1.86911785   0.52877742   1.21783702   1.83799066]\n",
      "\n",
      "Algorithms converged to VFI solution (tol=1e-06): ['VFI', 'PFI']\n",
      "\n",
      "3. OPTIMAL CONSUMPTION POLICIES:\n",
      "--------------------------------------------------\n",
      "Optimal consumption for each wealth level:\n",
      "VFI         : [0 1 1 1 1 0 1 2 2] -> ['c=0', 'c=1', 'c=1', 'c=1', 'c=1', 'c=0', 'c=1', 'c=2', 'c=2']\n",
      "PFI         : [0 1 1 1 1 0 1 2 2] -> ['c=0', 'c=1', 'c=1', 'c=1', 'c=1', 'c=0', 'c=1', 'c=2', 'c=2']\n",
      "Howard(m=1) : [0 1 1 1 1 0 1 2 2] -> ['c=0', 'c=1', 'c=1', 'c=1', 'c=1', 'c=0', 'c=1', 'c=2', 'c=2']\n",
      "Howard(m=3) : [0 1 1 1 1 0 1 2 2] -> ['c=0', 'c=1', 'c=1', 'c=1', 'c=1', 'c=0', 'c=1', 'c=2', 'c=2']\n",
      "Howard(m=5) : [0 1 1 1 1 0 1 2 2] -> ['c=0', 'c=1', 'c=1', 'c=1', 'c=1', 'c=0', 'c=1', 'c=2', 'c=2']\n",
      "\n",
      "All policies identical: True\n",
      "\n",
      "4. ECONOMIC INTERPRETATION:\n",
      "--------------------------------------------------\n",
      "Optimal Policy Analysis (using VFI results):\n",
      "  Wealth W=0: Consume c=0, Save s=0 (savings rate: 0.0%)\n",
      "  Wealth W=1: Consume c=1, Save s=0 (savings rate: 0.0%)\n",
      "  Wealth W=2: Consume c=1, Save s=1 (savings rate: 50.0%)\n",
      "  Wealth W=3: Consume c=1, Save s=2 (savings rate: 66.7%)\n",
      "  Wealth W=4: Consume c=1, Save s=3 (savings rate: 75.0%)\n",
      "  Wealth W=5: Consume c=0, Save s=5 (savings rate: 100.0%)\n",
      "  Wealth W=6: Consume c=1, Save s=5 (savings rate: 83.3%)\n",
      "  Wealth W=7: Consume c=2, Save s=5 (savings rate: 71.4%)\n",
      "  Wealth W=8: Consume c=2, Save s=6 (savings rate: 75.0%)\n",
      "\n",
      "Key Economic Insights:\n",
      "• Return on savings: 110.0% per period\n",
      "• Discount factor: 0.9 (agent values future at 90% of present)\n",
      "• Utility function: logarithmic u(c) = log(c + 0.1)\n",
      "• Average marginal propensity to consume: 0.250\n",
      "• Average marginal propensity to save: 0.750\n",
      "\n",
      "5. WEALTH DYNAMICS SIMULATION:\n",
      "--------------------------------------------------\n",
      "Simulating wealth evolution under optimal policy:\n",
      "Starting wealth: W=3\n",
      "Period 1: W=3 -> consume c=1, save s=2 -> W'=2\n",
      "Period 2: W=2 -> consume c=1, save s=1 -> W'=1\n",
      "Period 3: W=1 -> consume c=1, save s=0 -> W'=0\n",
      "Period 4: W=0 -> consume c=0, save s=0 -> W'=0\n",
      "Period 5: W=0 -> consume c=0, save s=0 -> W'=0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SOLVING CONSUMPTION-SAVINGS MDP WITH THREE ALGORITHMS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Value Function Iteration (VFI)\n",
    "print(\"\\n1. Value Function Iteration (VFI)\")\n",
    "print(\"-\" * 40)\n",
    "start_time = time.time()\n",
    "vfi_econ = VFI(econ_mdp, tol=1e-8, max_iter=1000)\n",
    "V_vfi_econ, pi_vfi_econ, iterations_vfi_econ = vfi_econ.solve()\n",
    "time_vfi_econ = time.time() - start_time\n",
    "\n",
    "print(f\"Iterations: {iterations_vfi_econ}\")\n",
    "print(f\"Time: {time_vfi_econ:.6f} seconds\")\n",
    "print(f\"Value function: {V_vfi_econ}\")\n",
    "print(f\"Optimal consumption policy: {pi_vfi_econ}\")\n",
    "\n",
    "# 2. Policy Function Iteration (PFI)\n",
    "print(\"\\n2. Policy Function Iteration (PFI)\")\n",
    "print(\"-\" * 40)\n",
    "start_time = time.time()\n",
    "pfi_econ = PFI(econ_mdp, eval_tol=1e-8, max_eval_iter=1000, max_outer=100)\n",
    "V_pfi_econ, pi_pfi_econ = pfi_econ.solve()\n",
    "time_pfi_econ = time.time() - start_time\n",
    "\n",
    "print(f\"Time: {time_pfi_econ:.6f} seconds\")\n",
    "print(f\"Value function: {V_pfi_econ}\")\n",
    "print(f\"Optimal consumption policy: {pi_pfi_econ}\")\n",
    "\n",
    "# 3. Howard PFI with different m values\n",
    "m_values = [1, 3, 5]\n",
    "howard_results = {}\n",
    "\n",
    "for m in m_values:\n",
    "    print(f\"\\n3.{m}. Howard PFI (m={m})\")\n",
    "    print(\"-\" * 40)\n",
    "    start_time = time.time()\n",
    "    howard_pfi_econ = HowardPFI(econ_mdp, m=m, max_outer=100)\n",
    "    V_howard_econ, pi_howard_econ = howard_pfi_econ.solve()\n",
    "    time_howard_econ = time.time() - start_time\n",
    "    \n",
    "    howard_results[m] = {\n",
    "        'V': V_howard_econ,\n",
    "        'pi': pi_howard_econ,\n",
    "        'time': time_howard_econ\n",
    "    }\n",
    "    \n",
    "    print(f\"Time: {time_howard_econ:.6f} seconds\")\n",
    "    print(f\"Value function: {V_howard_econ}\")\n",
    "    print(f\"Optimal consumption policy: {pi_howard_econ}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ECONOMIC ANALYSIS OF RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Collect all results for comparison\n",
    "algorithms_econ = [\"VFI\", \"PFI\"] + [f\"Howard(m={m})\" for m in m_values]\n",
    "times_econ = [time_vfi_econ, time_pfi_econ] + [howard_results[m]['time'] for m in m_values]\n",
    "value_functions_econ = [V_vfi_econ, V_pfi_econ] + [howard_results[m]['V'] for m in m_values]\n",
    "policies_econ = [pi_vfi_econ, pi_pfi_econ] + [howard_results[m]['pi'] for m in m_values]\n",
    "\n",
    "print(\"\\n1. ALGORITHM PERFORMANCE COMPARISON:\")\n",
    "print(\"-\" * 50)\n",
    "for i, (alg, t) in enumerate(zip(algorithms_econ, times_econ)):\n",
    "    print(f\"{alg:12s}: {t:.6f} seconds\")\n",
    "\n",
    "print(f\"\\nFastest: {algorithms_econ[np.argmin(times_econ)]} ({min(times_econ):.6f}s)\")\n",
    "print(f\"Slowest: {algorithms_econ[np.argmax(times_econ)]} ({max(times_econ):.6f}s)\")\n",
    "\n",
    "print(\"\\n2. VALUE FUNCTION CONVERGENCE:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Value functions (lifetime utility for each wealth level):\")\n",
    "for i, (alg, V) in enumerate(zip(algorithms_econ, value_functions_econ)):\n",
    "    print(f\"{alg:12s}: {V}\")\n",
    "\n",
    "# Check convergence\n",
    "tolerance = 1e-6\n",
    "converged_algorithms = []\n",
    "for i, V in enumerate(value_functions_econ):\n",
    "    if np.allclose(V, V_vfi_econ, atol=tolerance):\n",
    "        converged_algorithms.append(algorithms_econ[i])\n",
    "\n",
    "print(f\"\\nAlgorithms converged to VFI solution (tol={tolerance}): {converged_algorithms}\")\n",
    "\n",
    "print(\"\\n3. OPTIMAL CONSUMPTION POLICIES:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Optimal consumption for each wealth level:\")\n",
    "for i, (alg, pi) in enumerate(zip(algorithms_econ, policies_econ)):\n",
    "    consumption_policy = [f\"c={c}\" for c in pi]\n",
    "    print(f\"{alg:12s}: {pi} -> {consumption_policy}\")\n",
    "\n",
    "# Check if policies are identical\n",
    "policies_identical = all(np.array_equal(pi_vfi_econ, pi) for pi in policies_econ[1:])\n",
    "print(f\"\\nAll policies identical: {policies_identical}\")\n",
    "\n",
    "print(\"\\n4. ECONOMIC INTERPRETATION:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Optimal Policy Analysis (using VFI results):\")\n",
    "for w in range(len(pi_vfi_econ)):\n",
    "    optimal_c = pi_vfi_econ[w]\n",
    "    savings = w - optimal_c\n",
    "    savings_rate = (savings / w * 100) if w > 0 else 0\n",
    "    print(f\"  Wealth W={w}: Consume c={optimal_c}, Save s={savings} (savings rate: {savings_rate:.1f}%)\")\n",
    "\n",
    "print(f\"\\nKey Economic Insights:\")\n",
    "print(f\"• Return on savings: {R_return:.1%} per period\")\n",
    "print(f\"• Discount factor: {beta} (agent values future at {beta*100:.0f}% of present)\")\n",
    "print(f\"• Utility function: logarithmic u(c) = log(c + {epsilon})\")\n",
    "\n",
    "# Calculate marginal propensity to consume\n",
    "mpc_values = []\n",
    "for w in range(1, len(pi_vfi_econ)):\n",
    "    if w > 0:\n",
    "        mpc = (pi_vfi_econ[w] - pi_vfi_econ[w-1]) / 1  # Change in consumption per unit wealth\n",
    "        mpc_values.append(mpc)\n",
    "\n",
    "if mpc_values:\n",
    "    avg_mpc = np.mean(mpc_values)\n",
    "    print(f\"• Average marginal propensity to consume: {avg_mpc:.3f}\")\n",
    "    print(f\"• Average marginal propensity to save: {1-avg_mpc:.3f}\")\n",
    "\n",
    "print(\"\\n5. WEALTH DYNAMICS SIMULATION:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Simulating wealth evolution under optimal policy:\")\n",
    "initial_wealth = 3\n",
    "current_wealth = initial_wealth\n",
    "print(f\"Starting wealth: W={current_wealth}\")\n",
    "\n",
    "for period in range(5):\n",
    "    if current_wealth < len(pi_vfi_econ):\n",
    "        optimal_consumption = pi_vfi_econ[current_wealth]\n",
    "        savings = current_wealth - optimal_consumption\n",
    "        next_wealth = min(int(np.round(savings * R_return)), W_max)\n",
    "        \n",
    "        print(f\"Period {period+1}: W={current_wealth} -> consume c={optimal_consumption}, save s={savings} -> W'={next_wealth}\")\n",
    "        current_wealth = next_wealth\n",
    "    else:\n",
    "        print(f\"Period {period+1}: Wealth exceeds model bounds\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6515c266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
